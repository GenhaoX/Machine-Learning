{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  梯度提升树的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*class* `sklearn.ensemble.GradientBoostingClassifier`(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "*class* `sklearn.ensemble.GradientBoostingRegressor`(*, loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与随机森林一样，由于GBDT超参数数量较多，因此我们可以将GBDT的参数分为以下5大类别，其中标注为绿色的参数包括了我们未曾学过的知识、需要重点讲解：\n",
    "\n",
    "|类型|参数/属性|\n",
    "|---|---|\n",
    "|**迭代过程**|参数：n_estimators, learning_rate, **<font color=\"green\">loss, alpha, init</font>**<br>属性：<font color=\"green\">**loss_, init_, estimators_**</font>|\n",
    "|**弱评估器结构**|<font color=\"green\">**criterion**</font>, max_depth, min_samples_split, min_samples_leaf, <br>min_weight_fraction_leaf, max_leaf_nodes,<br>min_impurity_decrease|\n",
    "|**提前停止**|参数：<font color=\"green\">**validation_fraction, n_iter_no_change, tol**</font><br>属性：<font color=\"green\">**n_estimators_**</font>|\n",
    "|**弱评估器的训练数据**|参数：subsample, max_features, random_state<br>属性：<font color=\"green\">**oob_improvement, train_score_**</font>|\n",
    "|**其他**|ccp_alpha, warm_start|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 迭代过程\n",
    "\n",
    "之前我们提到过，GBDT的整体建模流程与AdaBoost高度相似，因此GBDT当中也有设置具体迭代次数（弱评估器次数）的参数`n_estimators`与学习率参数`learning_rate`，这两个参数的含义、以及对集成算法的影响与AdaBoost当中完全一致。\n",
    "\n",
    "具体地来说，对于样本$x_i$，集成算法当中一共有$T$棵树，则参数`n_estimators`的取值为T。假设现在正在建立第$t$个弱评估器，则则第$t$个弱评估器上$x_i$的结果可以表示为$f_t(x_i)$。假设整个Boosting算法对样本$x_i$输出的结果为$H(x_i)$，则该结果一般可以被表示为t=1~t=T过程当中，所有弱评估器结果的加权求和：\n",
    "\n",
    "$$H(x_i) =  \\sum_{t=1}^\\\n",
    "boldsymbol{\\color{red}T}\n",
    "\\phi_tf_t(x_i)\n",
    "$$\n",
    "\n",
    "其中，$\\phi_t$为第t棵树的权重。对于第$t$次迭代来说，则有：\n",
    "\n",
    "$$H_t(x_i) = H_{t-1}(x_i) + \\phi_tf_t(x_i)\n",
    "$$\n",
    "\n",
    "在这个一般过程中，每次将本轮建好的决策树加入之前的建树结果时，可以在权重$\\phi$前面增加参数$\\color{red}\\eta$，表示为第t棵树加入整体集成算法时的学习率，对标参数`learning_rate`。\n",
    "\n",
    "$$H_t(x_i) = H_{t-1}(x_i) + \\boldsymbol{\\color{red}\\eta} \\phi_tf_t(x_i)$$\n",
    "\n",
    "该学习率参数控制Boosting集成过程中$H(x_i)$的增长速度，是相当关键的参数。当学习率很大时，$H(x_i)$增长得更快，我们所需的`n_estimators`更少，当学习率较小时，$H(x_i)$增长较慢，我们所需的`n_estimators`就更多，因此boosting算法往往会需要在`n_estimators`与`learning_rate`中做出权衡。\n",
    "\n",
    "这两个参数的使用方法与AdaBoost中也完全一致，故此处不再赘述，后续我们会直接使用这两个参数进行调参。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 初始预测结果$H_0$的设置\n",
    "\n",
    "在上述过程中，我们建立第一个弱评估器时有：\n",
    "\n",
    "$$H_1(x_i) = H_{0}(x_i) + \\phi_1f_1(x_i)$$\n",
    "\n",
    "由于没有第0棵树的存在，因此$H_0(x_i)$的值在数学过程及算法具体实现过程中都需要进行单独的确定，这一确定过程由参数`init`确定。\n",
    "\n",
    "- 参数`init`：输入计算初始预测结果$H_0$的估计器对象。\n",
    "\n",
    "在该参数中，可以输入任意评估器、字符串\"zero\"、或者None对象，默认为None对象。\n",
    "> 当输入任意评估器时，评估器必须要具备fit以及predict_proba功能，即我们可以使用决策树、逻辑回归等可以输出概率的模型。如果输出一个已经训练过、且精细化调参后的模型，将会给GBDT树打下坚实的基础。<br><br>\n",
    "> 填写为字符串\"zero\"，则代表令$H_0 = 0$来开始迭代。<br><br>\n",
    "> 不填写，或填写为None对象，sklearn则会自动选择类`DummyEstimator`中的某种默认方式进行预测作为$H_0$的结果。`DummyEstimator`类是sklearn中设置的使用超简单规则进行预测的类，其中最常见的规则是直接从训练集标签中随机抽样出结果作为预测标签，也有选择众数作为预测标签等选项。\n",
    "\n",
    "一般在GBDT类的使用过程中，我们不会主动调节参数`init`，但是当我们有足够的算力支持超参数搜索时，我们可以在`init`上进行选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib as mlp\n",
    "import seaborn as sns\n",
    "import re, pip, conda\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.ensemble import AdaBoostRegressor as ABR\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "data = pd.read_csv(r\"D:\\myJupyter\\机器学习\\datasets\\House Price\\train_encode.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    7.2s remaining:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    7.3s finished\n"
     ]
    }
   ],
   "source": [
    "#回归数据\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]\n",
    "\n",
    "#定义所需的交叉验证方式\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=1412)\n",
    "\n",
    "def RMSE(result,name):\n",
    "    return abs(result[name].mean())\n",
    "\n",
    "gbr = GBR(random_state=1412) #实例化\n",
    "result_gbdt = cross_validate(gbr,X,y,cv=cv\n",
    "                             ,scoring=\"neg_root_mean_squared_error\" #负根均方误差\n",
    "                             ,return_train_score=True\n",
    "                             ,verbose=True\n",
    "                             ,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0.0\n",
      "42065.93924112058\n",
      "\n",
      "\n",
      "5669.291478825804\n",
      "27205.38818750095\n",
      "\n",
      "\n",
      "13990.791639702458\n",
      "28739.882050269225\n",
      "\n",
      "\n",
      "13990.790813889864\n",
      "28783.954343252786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "\n",
    "tree_reg = DTR(random_state=1412)\n",
    "rf = RFR(n_estimators=89, max_depth=22, max_features=14, min_impurity_decrease=0\n",
    "              ,random_state=1412, verbose=False, n_jobs=-1)\n",
    "\n",
    "for init in [tree_reg,rf,\"zero\",None]:\n",
    "    reg = GBR(init = init,random_state=1412)\n",
    "    cv = KFold(n_splits=5,shuffle=True,random_state=1412)\n",
    "    result_reg = cross_validate(reg,X,y,cv=cv,scoring=\"neg_root_mean_squared_error\"\n",
    "                                ,return_train_score=True\n",
    "                                ,verbose=False\n",
    "                                ,n_jobs=-1)\n",
    "    print(\"\\n\")\n",
    "    print(RMSE(result_reg,\"train_score\"))\n",
    "    print(RMSE(result_reg,\"test_score\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难发现，初始参数的具体输入会对模型的最终结果造成巨大影响，在init中输入训练好的模型会加重GBDT的过拟合，但同时也可能得到更好的测试集结果。我们甚至可以无限套娃，让init参数中输入被训练好的GBDT模型，当然，这样做的结果往往是过拟合被放大到无法挽回了。通常来说，我们还是会选择\"zero\"作为init的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与参数`init`相对的属性就是`init_`，当模型被拟合完毕之后，我们可以使用该属性来返回输出$H_0$的评估器对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyRegressor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = GBR(init = None,random_state=1412)\n",
    "reg.fit(X,y).init_ #返回sklearn中的玩具评估器DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=22, max_features=14, min_impurity_decrease=0,\n",
       "                      n_estimators=89, n_jobs=-1, random_state=1412,\n",
       "                      verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = GBR(init = rf,random_state=1412)\n",
    "reg.fit(X,y).init_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，在init中的值是我们自己输入的值的情况下，属性`init_`略显鸡肋，但我们或许会预见需要该属性的具体场景，例如在建模过程中进行监控打印时、或在大量初始化模型中选择最佳初始化模型时。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 使用回归器完成分类任务\n",
    "\n",
    "GBDT与AdaBoost及随机森林的关键区别之一，是GBDT中所有的弱评估器都是回归树，因此在实际调用梯度提升树完成分类任务时，需要softmax函数或sigmoid函数对回归树输出的结果进行处理。因此，对于二分类情况来说，集成算法对样本$x_i$输出的结果为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(x_i) =  \\sum_{t=1}^\\\n",
    "boldsymbol\n",
    "{\\color{red}T}\n",
    "\\phi_tf_t(x_i)$$\n",
    "\n",
    "$$ p(\\hat{y}_i = 1 |x_i) = \\sigma(H(x_i))$$\n",
    "\n",
    "其中$\\sigma$是sigmoid函数，当$p(\\hat{y}_i = 1 |x_i)$大于0.5时，样本$x_i$的预测类别为1，反之则为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而对多分类来说，情况就比较复杂了。在讲解AdaBoost时我们说明过，二分类当中我们只需求解一个概率$P(Y=1)$，因为$P(Y=0) = 1 - P(Y=1)$，因此$P(Y=1)$大于0.5时预测标签为1，否则预测标签为0。但在多分类当中，我们必须求解出所有标签类别所对应的概率，在所有这些概率当中，最大概率所对应的标签才是多分类的预测标签。GBDT对于多分类也只能输出集成算法回归结果$H(x_i)$，因此我们需要使用**softmax函数**帮助我们将回归值转化为概率，而**Softmax函数是接受K个连续型结果，并输出K个相对概率的函数**。\n",
    "\n",
    "一般我们在使用softmax函数时，3分类问题则需要向softmax函数输入3个值，4分类问题则需要向softmax函数输入4个值，以此类推，最终softmax函数输出的是与输入值同等数量的相对概率，而多分类算法的预测标签是相对概率最高的类别。因此，在使用softmax函数前，我们需要准备好与类别数量相当的$H(x_i)$。\n",
    "\n",
    "具体来说，当现在的问题是$K$分类、且每个类别为$[1,2,3...k]$时，我们则分别按照$y = 1, y = 2,...,y = k$进行建模，总共建立$K$棵树，每棵树输出的结果为：\n",
    "\n",
    "$$H^1(x_i), H^2(x_i),...,H^k(x_i)$$\n",
    "\n",
    "总共$K$个输出结果。然后，我们分别将$H^1(x_i)$到$H^k(x_i)$的结果输入softmax，来计算出每个标签类别所对应的概率。具体地来说，softmax函数的表达式为：\n",
    "\n",
    "$$Softmax(H^k(x)) = \\frac{e^{H^k(x)}}{\\sum_{k=1}^Ke^{H_k(x)}}$$\n",
    "\n",
    "其中$e$为自然常数，$H$是集成算法的输出结果，$K$表示标签中的类别总数为$K$，如三分类时$K=3$，四分类时$K=4$，$k$表示任意标签类别，$H_k$则表示以类别$k$为真实标签进行训练而得出的$H$。不难发现，Softmax函数的分子时多分类状况下**某一个标签类别的H(x)的指数函数**，而分母时多分类状况下**所有标签类别的H(x)的指数函数之和**，因此Softmax函数的结果代表了样本的预测标签为类别$k$的概率。假设现在是三分类[1,2,3]，则样本$i$被分类为1类的概率为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p^1(x_i) &= \\frac{e^{H^1(x)}}{\\sum_{k=1}^Ke^{H_k(x)}} \\\\\n",
    "&= \\frac{e^{H^1(x)}}{e^{H^1(x)}+e^{H^2(x)}+e^{H^3(x)}}\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "最终得到$K$个相对概率$p^k(x_i)$，并求解出相对概率最高的类别。不难发现，当执行多分类时，这一计算流程中涉及到的计算量以及弱评估器数量都会远远超出二分类以及回归类问题。实际上，在执行多分类任务时，如果我们要求模型迭代10次，模型则会按照实际的多分类标签数n_classes建立10 * n_classes个弱评估器。对于这一现象，我们可以通过属性`n_estimators_`以及属性`estimators_`查看到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_estimators_`：实际迭代次数，`estimators_`：实际建立的弱评估器数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GBC(n_estimators=10 #迭代次数为10次\n",
    "          ,random_state=1412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 79)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分类数据\n",
    "X_clf = data.iloc[:,:-2]\n",
    "y_clf = data.iloc[:,-2]\n",
    "X_clf.shape #查看X与y的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_clf) #6分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.fit(X_clf,y_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf.n_estimators_ #实际迭代数量为10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf.estimators_.shape #但每次迭代时其实建立了6个评估器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                             random_state=RandomState(MT19937) at 0x204E8C6B140),\n",
       "       DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                             random_state=RandomState(MT19937) at 0x204E8C6B140),\n",
       "       DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                             random_state=RandomState(MT19937) at 0x204E8C6B140),\n",
       "       DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                             random_state=RandomState(MT19937) at 0x204E8C6B140),\n",
       "       DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                             random_state=RandomState(MT19937) at 0x204E8C6B140),\n",
       "       DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                             random_state=RandomState(MT19937) at 0x204E8C6B140)],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf.estimators_[0] #其中一次迭代中建立的全部评估器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果是二分类则不会出现这种现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clf2 = load_breast_cancer().data\n",
    "y_clf2 = load_breast_cancer().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.unique(y_clf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GBC(n_estimators=10,random_state=1412)\n",
    "clf = clf.fit(X_clf2,y_clf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf.n_estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                              random_state=RandomState(MT19937) at 0x204E9485840)],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                              random_state=RandomState(MT19937) at 0x204E9485840)],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                              random_state=RandomState(MT19937) at 0x204E9485840)],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                              random_state=RandomState(MT19937) at 0x204E9485840)],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                              random_state=RandomState(MT19937) at 0x204E9485840)],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                              random_state=RandomState(MT19937) at 0x204E9485840)],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                              random_state=RandomState(MT19937) at 0x204E9485840)],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                              random_state=RandomState(MT19937) at 0x204E9485840)],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                              random_state=RandomState(MT19937) at 0x204E9485840)],\n",
       "       [DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
       "                              random_state=RandomState(MT19937) at 0x204E9485840)]],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一现象只在弱评估器为回归器的各类boosting算法中出现，对于弱评估器可以是回归树也可以是分类树的随机森林、Adaboost来说，多分类时每个类别对应的概率是在叶子节点上自然生成的。因为有此区别，因此多分类问题在随机森林上的计算可能会表现得更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 GBDT的8种损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为基于AdaBoost改进的Boosting算法，GBDT的功绩之一是将损失函数从有限的指数损失、MSE等推广到了任意可微函数，因此GBDT的损失函数选择异常丰富，因此我们可以在调参时加入损失函数作为需要调整的参数进行考量。在sklearn中，控制具体损失函数的参数为`loss`。\n",
    "\n",
    "GBDT中的损失函数因GBDT具体执行的预测任务而存在区别，同时也因标签的分布而存在区别。对于梯度提升分类树来说，loss的备选项有如下几种：\n",
    "\n",
    "- 分类器中的`loss`：字符串型，可输入\"deviance\", \"exponential\"，默认值=\"deviance\"\n",
    "\n",
    "其中\"deviance\"直译为偏差，特指逻辑回归的损失函数——交叉熵损失，而\"exponential\"则特指AdaBoost中使用的指数损失函数。对任意样本$i$而言，$y_i$为真实标签，$\\hat{y_i}$为预测标签，$H(x_i)$为集成算法输出结果，$p(x_i)$为基于$H(x_i)$和sigmoid/softmax函数计算的概率值。则各个损失的表达式为：\n",
    "\n",
    "**二分类交叉熵损失**——<br><br>\n",
    "$$L = -\\left( y\\log p(x) + (1 - y)\\log(1 - p(x)) \\right)$$\n",
    "\n",
    "<br>注意，log当中输入的一定是概率值。对于逻辑回归来说，概率就是算法的输出，因此我们可以认为逻辑回归中$p = H(x)$，但对于GBDT来说，$p(x_i) = Sigmoid(H(x_i))$，这一点一定要注意。\n",
    "\n",
    "<br>\n",
    "\n",
    "**多分类交叉熵损失**，总共有K个类别——<br><br>\n",
    "\n",
    "$$L = -\\sum_{k=1}^Ky^*_k\\log(P^k(x))$$\n",
    "\n",
    "其中，$P^k(x)$是概率值，对于多分类GBDT来说，$p^k(x) = Softmax(H^k(x))$。$y^*$是由真实标签转化后的向量。例如，在3分类情况下，真实标签$y_i$为2时，$y^*$为[$y^*_{1}$,$y^*_{2}$,$y^*_{3}$]，取值分别为：\n",
    "\n",
    "|$y^*_{1}$|$y^*_{2}$|$y^*_{3}$|\n",
    "|:-:|:-:|:-:|\n",
    "|$0$|$1$|$0$|\n",
    "\n",
    "这一转化过程与AdaBoost中多分类指数损失中的转化高度相似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**二分类指数损失**——<br><br>\n",
    "$$L = e^{-yH(x)}\n",
    "$$<br>\n",
    "\n",
    "**多分类指数损失**，总共有K个类别——<br><br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L &=exp \\left( -\\frac{1}{K}\\boldsymbol{y^* · H^*(x)} \\right) \\\\ \n",
    "& = exp \\left( -\\frac{1}{K}(y^1H^1(x)+y^2H^2(x) \\ + \\  ... + y^kH^k(x)) \\right)\n",
    "\\end{aligned}\n",
    "$$<br>\n",
    "\n",
    "需要注意，指数损失中的$y^*$与交叉熵损失中的$y^*$不是同样的向量。我们已经在逻辑回归的章节中详解过交叉熵损失，在AdaBoost的章节当中详解过指数损失，因此这里便不再展开赘述了。需要注意的是，一般梯度提升分类器默认使用交叉熵损失，如果使用指数损失，则相当于执行没有权重调整的AdaBoost算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于梯度提升回归树来说，loss的备选项有如下几种：\n",
    "\n",
    "- 回归器中的`loss`：字符串型，可输入{\"squared_error\", \"absolute_error\", \"huber\", \"quantile\"}，默认值=\"squared_error\"\n",
    "\n",
    "其中'squared_error'是指回归的平方误差，'absolute_error'指的是回归的绝对误差，这是一个鲁棒的损失函数。'huber'是以上两者的结合。'quantile'则表示使用分位数回归中的弹球损失pinball_loss。对任意样本$i$而言，$y_i$为真实标签，$H(x_i)$为预测标签，则各个损失的表达式为：\n",
    "\n",
    "**平方误差**——<br><br>\n",
    "\n",
    "$$L = \\sum{(y_i - H(x_i))^2}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**绝对误差**——<br><br>\n",
    "\n",
    "$$L = \\sum{|y_i - H(x_i)|}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Huber损失**——\n",
    "\n",
    "$$L = \\sum{l(y_i,H(x_i))}$$\n",
    "\n",
    "其中$$l = \\begin{split} \n",
    "\\begin{cases}\\frac{1}{2}(y_i - H(x_i))^2, & |y_i - H(x_i)|\\leq\\alpha \\\\\n",
    "\\alpha(|y_i - H(x_i)|-\\frac{\\alpha}{2}),& |y_i - H(x_i)|>\\alpha \\end{cases}\\end{split}, \\space \\space \\alpha \\in (0, 1)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**quantile损失**——\n",
    "\n",
    "$$L = \\sum{l(y_i,H(x_i))}$$\n",
    "\n",
    "其中$$l = \\begin{split} \n",
    "\\begin{cases}\n",
    "    \\alpha (y_i - H(x_i)), & y_i - H(x_i) > 0 \\\\\n",
    "    0,    & y_i - H(x_i) = 0 \\\\\n",
    "    (1-\\alpha) (y_i - H(x_i)), & y_i - H(x_i) < 0\n",
    "\\end{cases}\\end{split}, \\space \\space \\alpha \\in (0, 1)$$\n",
    "\n",
    "其中$\\alpha$是需要我们自己设置的超参数，由参数`alpha`控制。在huber损失中，alpha是阈值，在quantile损失中，alpha用于辅助计算损失函数的输出结果，默认为0.9。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>===更新警告===</font>**<br>\n",
    "在sklearn1.0版本及后续版本当中，损失函数\"ls\"与\"lad\"被删除了，其中\"ls\"的功能被\"squared_error\"取代，而\"lad\"被\"absolute_error\"取代。如果你在运行代码时，发现你的参数默认值、参数名称与课件中不相同，或者在运行过程中出现报错、警告等现象，你可能需要更新你的sklearn。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "对于相同的样本、相同的差异，不同损失函数给出的损失值不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = 10\n",
    "Hx = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "绝对 - 2\n",
    "平方 - 4 - 差异>1，误差被放大，差异<1，误差是会被缩小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如何选择不同的损失函数？\n",
    "\n",
    "GBDT是工业应用最广泛的模型，工业数据大部分都极度偏态、具有长尾，因此GBDT必须考虑**离群值**带来的影响。数据中的离群值会极大程度地影响模型地构建，当离群值在标签当中、而我们是依赖于减小损失函数来逐渐构建算法时，这种影响会前所未有地大。**因此Boosting是天生更容易被离群值影响的模型、也更擅长学习离群值的模型。**<br><br>\n",
    "![](https://discourse-cloud-file-uploads.s3.dualstack.us-west-2.amazonaws.com/business6/uploads/analyticsvidhya/original/1X/c3af040fef3ddc8f2bb1b393d71700e9a638426e.PNG)\n",
    "![](https://discourse-cloud-file-uploads.s3.dualstack.us-west-2.amazonaws.com/business6/uploads/analyticsvidhya/original/1X/f3c5885defae011dd385462f1f3812ff24393105.PNG)\n",
    "\n",
    "举例来说，若离群值的标签为1000，大部分正常样本的标签在0.1~0.2之间，算法一定会异常努力地学习离群值的规律，因为将离群值预测错误会带来巨大的损失。在这种状况下，最终迭代出的算法可能是严重偏离大部分数据的规律的。同样，我们也会遇见很多离群值对我们很关键的业务场景：例如，电商中的金额离群用户可能是VIP用户，风控中信用分离群的用户可能是高风险用户，这种状况下我们反而更关注将离群值预测正确。不同的损失函数可以帮助我们解决不同的问题——\n",
    "\n",
    "- **当高度关注离群值、并且希望努力将离群值预测正确时，选择平方误差**<br><br>\n",
    "这在工业中是大部分的情况。在实际进行预测时，离群值往往比较难以预测，因此离群样本的预测值和真实值之间的差异一般会较大。MSE作为预测值和真实值差值的平方，会放大离群值的影响，会让算法更加向学习离群值的方向进化，这可以帮助算法更好地预测离群值。\n",
    "\n",
    "- **努力排除离群值的影响、更关注非离群值的时候，选择绝对误差**<br><br>\n",
    "MAE对一切样本都一视同仁，对所有的差异都只求绝对值，因此会保留样本差异最原始的状态。相比其MSE，MAE对离群值完全不敏感，这可以有效地降低GBDT在离群值上的注意力。\n",
    "\n",
    "- **试图平衡离群值与非离群值、没有偏好时，选择Huber或者Quantileloss**<br><br>\n",
    "Huberloss损失结合了MSE与MAE，在Huber的公式中，当预测值与真实值的差异大于阈值时，则取绝对值，小于阈值时，则取平方。在真实数据中，部分离群值的差异会大于阈值，部分离群值的差异会小于阈值，因此比起全部取绝对值的MAE，Huberloss会将部分离群值的真实预测差异求平方，相当于放大了离群值的影响（但这种影响又不像在MSE那样大）。因此HuberLoss是位于MSE和MAE之间的、对离群值相对不敏感的损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 属性`loss_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GBR(n_estimators=10,random_state=1412).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.ensemble._gb_losses.LeastSquaresError at 0x29e5d1b8a00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg.loss_ #返回具体的损失函数对象，而不会返回公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结一下，在整个迭代过程中，我们涉及到了如下参数及属性："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|类型|参数/属性|\n",
    "|---|---|\n",
    "|**迭代过程**|参数：<br><br>&emsp;n_estimators：集成算法中弱评估器数量，对Boosting算法而言为实际迭代次数<br><br>&emsp;learning_rate：Boosting算法中的学习率，影响弱评估器结果的加权求和过程<br><br>&emsp;<font color=\"green\">**loss, alpha**</font>：需要优化的损失函数，以及特定损失函数需要调节的阈值<br><br>&emsp;<font color=\"green\">**init**</font>：初始化预测结果$H_0$的设置<br><br>属性：<br><br>&emsp;<font color=\"green\">**loss_**</font>：返回具体的损失函数对象<br><br>&emsp;<font color=\"green\">**init_**</font>：返回具体的初始化设置<br><br>&emsp;<font color=\"green\">**estimators_**</font>：返回实际建立的评估器列表<br><br>&emsp;n_estimators_：返回实际迭代次数|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 弱评估器结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在讲解决策树时，我们已经系统地讲解过弱评估器相关的一系列减枝参数，而在讲解随机森林时，我们又明确了这些减枝参数如何影响集成算法。因此我们对于Boosting算法中控制弱评估器的参数可谓非常熟悉："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|类型|参数|\n",
    "|----|----|\n",
    "|**弱评估器结构**|<font color=\"green\">**criterion**</font>：弱评估器分枝时的不纯度衡量指标<br><br>max_depth：弱评估器被允许的最大深度，默认3<br><br>min_samples_split：弱评估器分枝时，父节点上最少要拥有的样本个数<br><br>min_samples_leaf：弱评估器的叶子节点上最少要拥有的样本个数<br><br>min_weight_fraction_leaf：当样本权重被调整时，叶子节点上最少要拥有的样本权重<br><br>max_leaf_nodes：弱评估器上最多可以有的叶子节点数量<br><br>min_impurity_decrease：弱评估器分枝时允许的最小不纯度下降量|\n",
    "\n",
    "这些参数在随机森林中的**用法与默认值**与决策树类`DecisionTreeRegressor`中完全一致，专门用于对决策树进行剪枝、控制单个弱评估器的结构，考虑到大家在决策树中已经充分掌握这些参数，我们不再对这些参数一一进行详细说明了。在这里，需要重点说明的有两部分内容，一部分梯度提升树中默认的弱评估器复杂度所带来的问题，另一部分则是梯度提升树独有的不纯度衡量指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 梯度提升树中的弱评估器复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `max_depth`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然我们非常熟悉控制弱评估器结构的各个参数，但在实际应用任意Boosting算法时，我们还需进一步了解算法在这些参数上的默认值，以了解该算法留给我们的调参余地是否较大。\n",
    "\n",
    "在随机森林中我们讲到，森林中任意控制过拟合的参数基本都处于“关闭状态”，例如`max_depth`的默认值为None，表示不限深度，`min_samples_splits`的默认值为2，等同于不限制分枝，因此随机森林中长出的树都是剪枝前的树，也因此当随机森林算法处于过拟合状态时，我们可以使用粗或精的方法对弱评估器进行大刀阔斧的剪枝，当随机森林中的树被剪掉之后，可以很好的限制过拟合。然而这种情况并不适用于任何集成算法，尤其是以AdaBoost为基础的Boosting算法一族。\n",
    "\n",
    "在原始AdaBoost理论中，AdaBoost中使用的弱分类器都是最大深度为1的树桩或最大深度为3的小树苗，因此基于AdaBoost改进的其他Boosting算法也有该限制，即默认弱评估器的最大深度一般是一个较小的数字。**对GBDT来说，无论是分类器还是回归器，默认的弱评估器最大深度都为3**，因此GBDT默认就对弱评估器有强力的剪枝机制。\n",
    "\n",
    "当随机森林处于过拟合状态时，还可通过降低弱评估器复杂度的手段控制过拟合，但GBDT等Boosting算法处于过拟合状态时，便只能从数据上下手控制过拟合了（例如，使用参数`max_features`，在GBDT中其默认值为None），毕竟当`max_depth`已经非常小时，其他精剪枝的参数如`min_impurity_decrease`一般发挥不了太大的作用。\n",
    "\n",
    "也因此，通常认为Boosting算法比Bagging算法更不容易过拟合，一般在相似的数据上，Boosting算法表现出的过拟合程度会较轻。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 弗里德曼均方误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不纯度衡量指标`criterion`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criterion是树分枝时所使用的不纯度衡量指标。在sklearn当中，GBDT中的弱学习器$f$是CART树，因此每棵树在建立时都依赖于CART树分枝的规则进行建立。CART树每次在分枝时都只会分为两个叶子节点（二叉树），它们被称为左节点(left)和右节点(right)。在CART树中进行分枝时，我们需要找到令左右节点的不纯度之和最小的分枝方式。**通常来说，我们求解父节点的不纯度与左右节点不纯度之和之间的差值，这个差值被称为不纯度下降量**(impurity decrease)。不纯度的下降量越大，该分枝对于降低不纯度的贡献越大。\n",
    "\n",
    "对GBDT来说，不纯度的衡量指标有2个：弗里德曼均方误差friedman_mse与平方误差squared_error。其中平方误差我们非常熟悉，弗里德曼均方误差是由Friedman在论文《贪婪函数估计：一种梯度提升机器》（GREEDY FUNCTION APPROXIMATION: \n",
    "A GRADIENT BOOSTING MACHINE）中提出的全新的误差计算方式。遗憾的是，在论文当中，Friedman并没有提供弗里德曼均方误差的公式本身，而只提供了使用弗里德曼均方误差之后推导出的不纯度下降量的公式。该公式如下：\n",
    "\n",
    "**<center>基于弗里德曼均方误差的不纯度下降量</center>**<br>\n",
    "\n",
    "$$\\frac{w_lw_r}{w_l \\space + \\space w_r} * \\left( \\frac{\\sum_l{(r_i - \\hat{y_i})^2}}{w_l} - \\frac{\\sum_r{(r_i - \\hat{y_i})^2}}{w_r}\\right)^2$$\n",
    "\n",
    "<br>\n",
    "\n",
    "其中$w$是左右叶子节点上的样本量，当我们对样本有权重调整时，$w$则是叶子节点上的样本权重。$r_i$大多数时候是样本i上的残差（父节点中样本i的预测结果与样本i的真实标签之差），也可能是其他衡量预测与真实标签差异的指标，$\\hat{y_i}$是样本i在当前子节点下的预测值。所以这个公式其实可以解读成：\n",
    "\n",
    "**<center>左右叶子节点上样本量的调和平均 * (左叶子节点上均方误差 - 右叶子节点上的均方误差)^2</center>**\n",
    "\n",
    "根据论文中的描述，弗里德曼均方误差使用调和平均数（分子上相乘分母上相加）来控制左右叶子节点上的样本数量，相比普通地求均值，调和平均必须在左右叶子节点上的样本量/样本权重相差不大的情况下才能取得较大的值（F1 score也是用同样的方式来调节Precision和recall）。这种方式可以令不纯度的下降得更快，让整体分枝的效率更高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，在决策树进行分枝时，一般不太可能直接将所有样本分成两个不纯度非常低的子集（分别位于两片叶子上），相对的，树会偏向于建立一个不纯度非常非常低的子集，然后将剩下无法归入这个低不纯度子集的样本全部打包成另外一个子集。因此直接使用两个子集之间的MSE差距来衡量不纯度的下降量非常聪明，如果两个子集之间的MSE差异很大，则说明其中一个子集的MSE一定很小，对整体分枝来说是更有利的。同样非常遗憾的是，Friedman并没有在为我们提供完整数学证明，以佐证刚才所说的观点。\n",
    "\n",
    "除了Friedman_mse之外，我们也可以使用普通的平方误差作为不纯度的衡量。使用普通平方误差时，我们可以直接计算父节点的平方误差与子节点平方误差的加权求和之间的差异。\n",
    "\n",
    "**<center>平方误差的不纯度下降量</center>**\n",
    "\n",
    "$$\\frac{\\sum_p{(r_i - \\hat{y_i})^2}}{w_l + w_r} - (\\frac{w_l}{w_l+w_r} * \\sum_l{(r_i - \\hat{y_i})^2} + \\frac{w_r}{w_l+w_r} * \\sum_r{(r_i - \\hat{y_i})^2})\n",
    "$$<br>\n",
    "\n",
    "**大部分时候，使用弗里德曼均方误差可以让梯度提升树得到很好的结果**，因此GBDT的默认参数就是Friedman_mse。不过许多时候，我们会发现基于平方误差的分割与基于弗里德曼均方误差的分割会得到相同的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 梯度提升树的提前停止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在学习机器学习理论与方法时，我们极少提及迭代的提前停止问题。在机器学习中，依赖于迭代进行工作的算法并不算多，同时课程中的数据量往往也比较小，因此难以预见需要提前停止迭代以节省计算资源或时间的情况。但对于工业界使用最广泛的GBDT而言，提前停止是需要考虑的关键问题。\n",
    "\n",
    "对于任意需要迭代的算法，迭代的背后往往是损失函数的最优化问题。例如在逻辑回归中，我们在进行梯度下降的迭代时，是希望找到交叉熵损失函数的最小值；而在梯度提升树中，我们在一轮轮建立弱评估器过程中，也是希望找到对应损失函数的最小值。理想状态下，无论使用什么算法，只要我们能够找到损失函数上真正的最小值，那**模型就达到“收敛”状态，迭代就应该被停止**。\n",
    "\n",
    "然而遗憾的是，我们和算法都不知道损失函数真正的最小值是多少，而算法更不会在达到收敛状态时就自然停止。在机器学习训练流程中，我们往往是通过给出一个极限资源来控制算法的停止，比如，我们通过超参数设置允许某个算法迭代的最大次数，或者允许建立的弱评估器的个数。因此无论算法是否在很短时间内就锁定了足够接近理论最小值的次小值、或者算法早已陷入了过拟合状态、甚至学习率太低导致算法无法收敛，大多数算法都会持续（且无效地）迭代下去，直到我们给与的极限资源全部被耗尽。对于复杂度较高、数据量较大的Boosting集成算法来说，无效的迭代常常发生，因此作为众多Boosting算法的根基算法，梯度提升树自带了提前停止的相关超参数。另外，逻辑回归看起来会自然停止，是因为逻辑回归内置提前停止机制。\n",
    "\n",
    "我们根据以下原则来帮助梯度提升树实现提前停止：\n",
    "\n",
    "- **当GBDT已经达到了足够好的效果（非常接近收敛状态），持续迭代下去不会有助于提升算法表现**\n",
    "- **GBDT还没有达到足够好的效果（没有接近收敛），但迭代过程中呈现出越迭代算法表现越糟糕的情况**\n",
    "- **虽然GBDT还没有达到足够好的效果，但是训练时间太长/速度太慢，我们需要重新调整训练**\n",
    "\n",
    "第三种情况可以通过参数verbose打印结果来观察，如果GBDT的训练时间超过半个小时，建树平均时长超出1分钟，我们就可以打断训练考虑重调参数。前两种情况则比较复杂，我们首先必须理解什么叫做“足够好的效果”。在GBDT迭代过程中，只要损失函数的值持续减小、或验证集上的分数持续上升，我们就可以认为GBDT的效果还有提升空间。在实际训练过程中，刚开始训练时，测试集和训练集上的损失一般都很高（有时，训练集上的损失甚至比测试集上的损失还高，这说明模型严重欠训练），但随着训练次数的增多，两种损失都会开始快速下降，一般训练集下降得更快，测试集下降得缓慢。直到某一次迭代时，无论我们如何训练，测试集上的损失都不再下降，甚至开始升高，此时我们就需要让迭代停下。\n",
    "\n",
    "如下图所示，下图中横坐标为迭代次数，纵坐标为损失函数的值。当测试集上的损失不再下降、持续保持平稳时，满足条件1，继续训练会浪费训练资源，迭代下去模型也会停滞不前，因此需要停止（左图）。当测试集上的损失开始升高时，往往训练集上的损失还是在稳步下降，继续迭代下去就会造成训练集损失比测试集损失小很多的情况，也就是过拟合（右侧），此时满足条件2，也需要提前停止。在过拟合之前及时停止，能够防止模型被迭代到过拟合状况下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021MachineLearning/Ensembles/Public/53.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际数据训练时，我们往往不能动用真正的测试集进行提前停止的验证，因此我们需要从训练集中划分出一小部分数据，专用于验证是否应该提前停止。那我们如何找到这个验证集损失不再下降、准确率不再上升的“某一时间点”呢？此时，我们可以规定一个阈值，例如，**当连续`n_iter_no_change`次迭代中，验证集上损失函数的减小值都低于阈值`tol`，或者验证集的分数提升值都低于阈值`tol`的时候，我们就令迭代停止**。此时，即便我们规定的`n_estimators`或者`max_iter`中的数量还没有被用完，我们也可以认为算法已经非常接近“收敛”而将训练停下。这种机制就是提前停止机制Early Stopping。这种机制中，需要设置阈值`tol`，用于不断检验损失函数下降量的验证集，以及损失函数连续停止下降的迭代轮数`n_iter_no_change`。在GBDT当中，这个流程刚好由以下三个参数控制：\n",
    "\n",
    "- `validation_fraction`：从训练集中提取出、用于提前停止的验证数据占比，值域为[0,1]。\n",
    "- `n_iter_no_change`：当验证集上的损失函数值连续n_iter_no_change次没有下降或下降量不达阈值时，则触发提前停止。平时则设置为None，表示不进行提前停止。\n",
    "- `tol`：损失函数下降的阈值，默认值为1e-4，也可调整为其他浮点数来观察提前停止的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，当提前停止条件被触发后，梯度提升树会停止训练，即停止建树。因此，当提前停止功能被设置打开时，我们使用属性`n_estimators_`调出的结果很可能不足我们设置的`n_estimators`，属性`estimators_`中的树数量也可能变得更少："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1  5514149069.3942            0.59s\n",
      "         2  4757494791.3091            0.93s\n",
      "         3  4135611877.1608            0.90s\n",
      "         4  3617733865.9522            0.81s\n",
      "         5  3181720508.2060            0.79s\n",
      "         6  2815131775.8119            0.76s\n",
      "         7  2507962392.4188            0.73s\n",
      "         8  2238966663.4425            0.69s\n",
      "         9  2009699424.2492            0.67s\n",
      "        10  1814955565.7236            0.67s\n",
      "        20   822068581.4203            0.52s\n",
      "        30   509402412.7933            0.44s\n",
      "        40   386179875.0618            0.51s\n",
      "        50   321922360.8108            0.43s\n",
      "        60   283265211.0252            0.37s\n",
      "        70   255541395.6525            0.27s\n",
      "        80   231234309.1592            0.18s\n",
      "        90   210609110.6698            0.09s\n"
     ]
    }
   ],
   "source": [
    "reg1 = GBR(n_estimators=100\n",
    "          ,validation_fraction=0.1,n_iter_no_change=10,tol=0.01\n",
    "          ,random_state=1412,verbose=True).fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1  5414666697.2413            0.59s\n",
      "         2  4691239369.7631            0.78s\n",
      "         3  4086315332.5980            0.77s\n",
      "         4  3579387985.7262            0.79s\n",
      "         5  3153929839.6306            0.78s\n",
      "         6  2792392551.1292            0.73s\n",
      "         7  2497130392.1365            0.72s\n",
      "         8  2238910590.0126            0.69s\n",
      "         9  2017698809.4694            0.66s\n",
      "        10  1816369982.8554            0.64s\n",
      "        20   832375488.7016            0.52s\n",
      "        30   534539118.3102            0.45s\n",
      "        40   407622352.5644            0.38s\n",
      "        50   341113313.8828            0.33s\n",
      "        60   301354860.3006            0.28s\n",
      "        70   274573946.5769            0.20s\n",
      "        80   252246253.6291            0.13s\n",
      "        90   235659790.9783            0.07s\n",
      "       100   219637782.6059            0.00s\n"
     ]
    }
   ],
   "source": [
    "reg2 = GBR(n_estimators=100,random_state=1412,verbose=True).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg1.n_estimators_ #打开提前停止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg2.n_estimators_ #关闭提前停止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 提前停止pk不提前停止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.3,random_state=1412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5076413154602051\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "reg1 = GBR(n_estimators=1000\n",
    "          ,validation_fraction=0.2,n_iter_no_change=10,tol=0.001\n",
    "          ,random_state=1412).fit(Xtrain,Ytrain)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.888045787811279\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "reg2 = GBR(n_estimators=1000,random_state=1412).fit(Xtrain,Ytrain)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg1.n_estimators_ #打开提前停止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg2.n_estimators_ #关闭提前停止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.897711170231429"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg1.score(Xtest,Ytest) #R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89927123751538"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg2.score(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么时候使用提前停止呢？一般有以下几种场景：\n",
    "\n",
    "- 当数据量非常大，肉眼可见训练速度会非常缓慢的时候，开启提前停止以节约运算时间<br><br>\n",
    "- n_estimators参数范围极广、可能涉及到需要500~1000棵树时，开启提前停止来寻找可能的更小的n_estimators取值<br><br>\n",
    "- 当数据量非常小，模型很可能快速陷入过拟合状况时，开启提前停止来防止过拟合<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|类型|参数|\n",
    "|----|----|\n",
    "|**提前停止**|<font color=\"green\">**validation_fraction**</font>：从训练集中提取出、用于提前停止的验证数据占比<br><br><font color=\"green\">**n_iter_no_change**</font>：当验证集上的损失函数值连续n_iter_no_change次没有下降<br>或下降量不达阈值时，则触发提前停止<br><br><font color=\"green\">**tol**</font>：损失函数下降量的最小阈值|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 梯度提升树的袋外数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在讲解梯度提升树的基本原理时，我们提到梯度提升树结合了Boosting和Bagging中的重要思想。受到随机森林的启发，梯度提升树在每次建树之前，也允许模型对于数据和特征进行随机有放回抽样，构建与原始数据集相同数据量的自助集。在梯度提升树的原理当中，当每次建树之前进行随机抽样时，这种梯度提升树叫做随机提升树（Stochastic Gradient Boosting）。相比起传统的梯度提升树，随机提升树输出的结果往往方差更低，但偏差略高。如果我们发现GBDT的结果高度不稳定，则可以尝试使用随机提升树。\n",
    "\n",
    "在GBDT当中，对数据的随机有放回抽样比例由参数`subsample`确定，当该参数被设置为1时，则不进行抽样，直接使用全部数据集进行训练。当该参数被设置为(0,1)之间的数字时，则使用随机提升树，在每轮建树之前对样本进行抽样。对特征的有放回抽样比例由参数`max_features`确定，随机模式则由参数`random_state`确定，这两个参数在GBDT当中的使用规则都与随机森林中完全一致。\n",
    "\n",
    "需要注意的是，如果`subsample`<1，即存在有放回随机抽样时，当数据量足够大、抽样次数足够多时，大约会有37%的数据被遗漏在“袋外”（out of bag）没有参与训练。在随机森林课程当中，我们详细地证明了37%的由来，并且使用这37%的袋外数据作为验证数据，对随机森林的结果进行验证。在GBDT当中，当有放回随机抽样发生时，自然也存在部分袋外数据没有参与训练。这部分数据在GBDT中被用于对每一个弱评估器的建立结果进行验证。\n",
    "\n",
    "具体地来说，**每建立一棵树，GBDT就会使用当前树的袋外数据对建立新树后的模型进行验证，以此来对比新建弱评估器后模型整体的水平是否提高，并保留提升或下降的结果**。这个过程相当于在GBDT迭代时，不断检验损失函数的值并捕捉其变化的趋势。在GBDT当中，这些袋外分数的变化值被储存在属性`oob_improvement_`中，同时，GBDT还会在每棵树的训练数据上保留袋内分数（in-bag）的变化，且储存在属性`train_score_`当中。也就是说，即便在不做交叉验证的情况下，我们也可以简单地通过属性`oob_improvement`与属性`train_score_`来观察GBDT迭代的结果。我们来看具体的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GBR(n_estimators=500,learning_rate=0.1\n",
    "          ,subsample=0.3 #每次建树只抽取30%的数据进行训练\n",
    "          ,random_state=1412).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg.oob_improvement_.shape #袋外数据上的损失函数变化量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg.train_score_.shape #训练集上的损失函数变化量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(500),reg.oob_improvement_,color=\"red\",label=\"Validation\")\n",
    "plt.plot(range(500),reg.train_score_,color=\"k\",label=\"Train\")\n",
    "plt.title(\"Decrease in MSE\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难发现，随着迭代次数的增加，训练集上的损失函数一直有下降量，虽然下降量在逐渐变小，但是损失函数的确是在持续减下降的，相对的，验证集上的结果则在少有波动之后就维持在0附近不动了，也就是说模型的泛化能力在很早的时候就预见了瓶颈。如果我们开启提前停止，恐怕实际的迭代次数会远远少于我们给与的500次："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reg = GBR(n_estimators=500,learning_rate=0.1\n",
    "          ,tol=1e-6 #非常非常低的阈值\n",
    "          ,n_iter_no_change=5\n",
    "          ,validation_fraction = 0.3\n",
    "          ,subsample=0.3\n",
    "          ,random_state=1412).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg.oob_improvement_.shape #实际我只迭代了69次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(69),reg.oob_improvement_,color=\"red\",label=\"Validation\")\n",
    "plt.plot(range(69),reg.train_score_,color=\"k\",label=\"Train\")\n",
    "plt.title(\"Decrease in MSE with early stopping\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难发现，袋外数据的设置可以帮助我们快速把握模型的迭代情况，在当前数据集上，设置大约50以下的迭代次数，就足够使用了。因此在我们对GBDT进行超参数调优时，我们也有了天然的`n_estimators`的范围设置。当然，当我们调整其他参数（如`learning_rate`或者`max_depth`）之后，`n_estimators`的范围可能受到影响，但我们已经有了50这个可以参考的点。\n",
    "\n",
    "奇怪的是，袋外数据是天然的验证数据，而提前停止时需要使用验证集的功能，但sklearn中并未配置直接使用袋外数据来进行提前停止的功能。如果能够使用袋外数据进行提前停止，则可以使用更多数据进行训练，这将会更加有利于模型的学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|类型|参数|\n",
    "|----|----|\n",
    "|**弱评估器的训练数据**|参数：<br><br>&emsp;subsample：每次建树之前，从全数据集中进行有放回随机抽样的比例<br><br>&emsp;max_features：每次建树之前，从全特征中随机抽样特征进行分枝的比例<br><br>&emsp;random_state：随机数种子，控制整体随机模式<br><br>属性：<br><br>&emsp;<font color=\"green\">**oob_improvement**</font>：每次建树之后相对于上一次袋外分数的增减<br><br>&emsp;<font color=\"green\">**train_score_**</font>：每次建树之后相对于上一次验证时袋内分数的增减|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 缺失的class_weight与n_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|类型|参数/属性|\n",
    "|---|---|\n",
    "|**迭代过程**|参数：n_estimators, learning_rate, <font color=\"green\">**loss, alpha, init**</font><br>属性：<font color=\"green\">**loss_, init_, estimators_**</font>|\n",
    "|**弱评估器结构**|<font color=\"green\">**criterion**</font>, max_depth, min_samples_split, min_samples_leaf, <br>min_weight_fraction_leaf, max_leaf_nodes,<br>min_impurity_decrease|\n",
    "|**提前停止**|参数：<font color=\"green\">**validation_fraction, n_iter_no_change, tol**</font><br>属性：<font color=\"green\">**n_estimators_**</font>|\n",
    "|**弱评估器的训练数据**|参数：subsample, max_features, random_state<br>属性：<font color=\"green\">**oob_improvement, train_score_**</font>|\n",
    "|**其他**|ccp_alpha, warm_start|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里，我们已经讲解完毕了梯度提升回归树以及梯度提升分类树中的所有参数。需要注意的是，作为最常用的集成算法之一，sklearn中的GBDT分类器并没有提供调节样本不均衡问题的参数class_weights，也不存在并行参数n_jobs。\n",
    "\n",
    "不在样本不均衡问题上做文章，或许跟GBDT的弱评估器都是回归器有关，又或许是因为GBDT拥有非常强的学习能力，因此不会轻易被样本不均衡问题左右，也可能是因为sklearn在配置GBDT时存在一些失误。但务必要注意，如果样本存在严重不均衡的状况，那我们可能会考虑不使用梯度提升树，或者先对数据进行样本均衡的预处理后，再使用梯度提升树。\n",
    "\n",
    "GBDT中的树必须一棵棵建立、且后面建立的树还必须依赖于之前建树的结果，因此GBDT很难在某种程度上实现并行，因此sklearn并没有提供n_jobs参数给Boosting算法使用。更加先进的Boosting算法们已经实现了分枝并行，但sklearn还无法实现这个功能，因此GBDT的计算速度难以得到加速，这是sklearn实现GBDT无法跨越的一个缺陷。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
